name				model 	n_epochs	print_every		plot_every	learning rate	data set		num_samples		num_hidden	combined	class weights	num_layers	rand sampling	
model.pt			RNN 	100 000		1000			1000		0.005			world-cities	countries>100	128			yes			yes				1			yes
LSTM_model.pt     	LSTM 	100 000 	1000			1000		0.005			world-cities	countries>100	128			no 			yes				1			yes
LSTM_model_2.pt  	LSTM 	100 000		1000			1000		0.005			world-cities	countries>100	128			yes			yes				1			yes
LSTM_model_3.pt  	LSTM 	100 000		1000			1000		0.005			geonames		countries>100	128			yes			yes				1			yes
LSTM_model_4.pt  	LSTM 	100 000		1000			1000		0.005			geonames		countries>300	128			yes			yes				1			yes
LSTM_model_5.pt  	LSTM 	100 000		1000			1000		0.005			geonames		countries>300	128			yes			no				1			yes
LSTM_model_6.pt  	LSTM 	100 000		1000			1000		0.005			geonames		countries>300	128			no			no				2			yes
LSTM_model_7.pt  	LSTM 	10			1000			1000		0.005			geonames		countries>300	128			no			yes				2			no	
LSTM_model_8.pt 	LSTM 	20		 	1000			1000		0.005			geonames		countries>300	128			no 			yes				2			no
GRU_model_8.pt 	 	GRU 	20		 	1000			1000		0.005			geonames		countries>300	128			no 			yes				2			no
GRU_model_9.pt 	 	GRU 	20		 	1000			1000		0.005			geonames		countries>300	128			no 			yes				2			no
GRU_model_10.pt	 	GRU 	 		 	5000			1000		0.005			geonames		countries>300	128			no 			yes				2			yes
GRU_model_10_2.pt	GRU 	1085000	 	5000			1000		0.005			geonames		countries>300	128			no 			yes				2			yes
GRU_model_11.pt 	GRU 	1090000		5000			1000		0.005			geonames		countries>300	128			no 			yes 			2			yes
GRU_model_12.pt 	GRU 	1630000		5000			1000		0.005			geonames		countries>300	128			no 			yes 			2			yes
GRU_model_13.pt 	GRU 	485000		5000			1000		0.001			geonames		countries>300	128			no 			yes 			2			yes
GRU_model_14.pt 	GRU 	1630000		5000			1000		0.005			geonames		countries>300	128			no 			no 				1			yes
GRU_model_15.pt 	GRU 	310000		5000			1000		0.01			geonames		countries>300	128			no 			no 				2			yes
GRU_model_16.pt 	GRU 				5000			1000		0.01			geonames		countries>300	64			no 			no 				3			yes


GRU_model_8_Adam.pt GRU     20          1000            1000        0.005           geonames        countries>300	128         no 			yes				2			no 

model_8.pt 			RNN 	20		 	1000			1000		0.005			geonames		countries>300	128			no 			yes				2			no
GRU_model_adagrad.pt GRU    1085000     5000            5000        0.005           geonames		countries>300	128			no 			yes				2			yes                        


evaluation

name 				time taken		avg f1-score	train acc		test acc	(on 10 000 samples)		
model.pt 							0.3				28.4			31.4
LSTM_model.pt 						0.18			21.6
LSTM_model_2.pt 					0.2				21.9
LSTM_model_3.pt 					0.04			6.4				7.5
LSTM_model_4.pt 					0.14			15				17
LSTM_model_5.pt 	24m				0.2				20.1			17.2
LSTM_model_6.pt 	22m				0.1				13.9			12.8
LSTM_model_7.pt 	121m		 	0.23			29				27
LSTM_model_8.pt 	387m			0.3 			38 				38
model_8.pt 			61m				0.09			12.8			12.8
GRU_model_8.pt 		~300m 			0.32			41				42
GRU_model_9.pt 		 				0.3								38
GRU_model_10.pt 		 			0.25			26.7			29.2
GRU_model_10_2.pt 		 			0.39			51				48.5
GRU_model_11.pt 	210m			0.36			45.1			44.5		-> did not improve rly
GRU_model_12.pt 	388m 			0.25			30.4			29			-> simply slower and not as good, it seems
GRU_model_13.pt 	204m			0.05			9.6				9.8 		-> weight decay AND dropout was not a good idea
GRU_model_14.pt 	336m			0.23			30.09			30.09		-> meh		
GRU_model_15.pt 	67m				0.38			44.3			44.3		-> seems good to use 0.01 as a learning rate, goes quicker and gives OK results

GRU_model_8_Adam.pt                 0.081754        11.7            10.2                                                     
GRU_model_adagrad.pt 257m           0.2319          24.5            26.0                                

things that changed for each model:

LSTM_model.pt 		-	introduced LSTM layer
LSTM_model_2.pt 	-	did combined input and hidden (as in the original rnn)
LSTM_model_3.pt 	-	new data set with more data points
LSTM_model_4.pt 	-	more filtered data
LSTM_model_5.pt 	-	no class weights
LSTM_model_6.pt 	-	two LSTM layers
LSTM_model_7.pt 	-	no random sampling - run through whole training set every epoch
LSTM_model_8.pt 	-	check error on validation set to protect against overfitting  (also introduced shuffling the data between every epoch)
model_8.pt 			-	did the same as LSTM_model_8.pt, but with "normal" RNN. when sseeing that the results were poor, redid it but with two layers
GRU_model_8.pt 		-	did the same as LSTM_model_8.pt, buth with GRU layers
GRU_model_9.pt 		-	introduce dropout
GRU_model_10.pt 	-	no random sampling + early stopping only save when val error is at its lowest
GRU_model_10_2.pt 	-	continued training GRU_model_10.pt, this time with softer criteria for early stopping. also only save when val error is at its lowest

GRU_model_11.pt 	-	+ use momentum
GRU_model_12.pt 	- 	other learning rate (0.001)
GRU_model_13.pt 	-	+ weight decay=0.01 
GRU_model_14.pt 	-	original setup, just more epochs (and still early stopping, only saving when necessary) -> accidentally saved it after the last epoch -> not good
GRU_model_15.pt 	-	other learning rate (0.01)
GRU_model_16.pt 	-	one more layer, less hidden nodes


could continue training GRU_model_10, but continue training it deterministically

TODO: play around with learning rate, number of layers and number of hidden nodes, use momentum, use dropout, use batch normalization, use learning rate decay
use batches instead of one sample at a time, to make it go faster
- different optimizer
- different loss/activation function
- try adagrad

